{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29427ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard ds libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# my acquire and prepare file\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "\n",
    "# import splitting functions\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2dee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets)\n",
    "# containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8031066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = data('iris')\n",
    "df_iris.info()\n",
    "# data('iris', show_doc=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f438ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 3 rows\n",
    "print(df_iris.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0022db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows and columns (shape)\n",
    "print(df_iris.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names\n",
    "print(df_iris.columns)\n",
    "o# or df_iris.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data type of each column\n",
    "print(df_iris.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2872f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics for each of the numeric variables\n",
    "print(df_iris.describe().T) #.T transposes it for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88d32b",
   "metadata": {},
   "source": [
    "### # 5. Read the data from this google sheet into a dataframe, df_google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from this google sheet into a dataframe, df_google.\n",
    "\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "\n",
    "df_google = pd.read_csv(csv_export_url)\n",
    "df_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d881e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the first 3 rows\n",
    "print(df_google.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef80ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows and columns\n",
    "print(df_google.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names\n",
    "print(df_google.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1667d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data type of each column\n",
    "print(df_google.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary statistics for each of the numeric variables\n",
    "print(df_google.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5825e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the unique values for each of your categorical variables\n",
    "df_google.Survived.value_counts(dropna=False)\n",
    "df_google.Name.value_counts(dropna=False)\n",
    "df_google.Sex.value_counts(dropna=False)\n",
    "df_google.Cabin.value_counts(dropna=False)\n",
    "df_google.Embarked.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e304d9dd",
   "metadata": {},
   "source": [
    "### 6. Download the previous exercise's file into an excel (File → Download → Microsoft Excel). Read the downloaded file into a dataframe named df_excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel(\"train.xlsx\", sheet_name='train')\n",
    "print(df_excel.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ded79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "df_excel_sample = df_excel.head(100)\n",
    "print(df_excel_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of rows of your original dataframe\n",
    "print(len(df_excel)) #or df_excel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb717e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 5 column names\n",
    "df_excel.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b41d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the column names that have a data type of object\n",
    "object_columns = df_excel.select_dtypes(include=['object']).head()\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the range for each of the numeric variables.\n",
    "titanic_stats = df_excel[['Age', 'Fare']].describe().T\n",
    "titanic_stats['range'] = titanic_stats['max'] -titanic_stats['min']\n",
    "titanic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece855a",
   "metadata": {},
   "source": [
    "#### Make a new python module, acquire.py to hold the following data aquisition functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91746d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function named get_titanic_data that returns the titanic data from the codeup\n",
    "# data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database.\n",
    "\n",
    "import env\n",
    "\n",
    "def get_titanic_data():\n",
    "    url = env.get_db_url('titanic_db')\n",
    "    \n",
    "    return pd.read_sql('SELECT * FROM passengers', url)\n",
    "\n",
    "get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813eca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function named get_iris_data that returns the data from the iris_db on the codeup\n",
    "# data science database as a pandas data frame. The returned data frame should include the \n",
    "# actual name of the species in addition to the species_ids. Obtain your data from the Codeup Data Science Database.\n",
    "from env import user, password, host\n",
    "\n",
    "#url = f'mysql+pymysql://{user}:{password}@{host}/iris_db'\n",
    "\n",
    "def get_iris_data():\n",
    "    from env import user, password, host\n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/iris_db'\n",
    "    url = env.get_db_url('iris_db')\n",
    "    return pd.read_sql('SELECT * FROM measurements JOIN iris_db.species USING(species_id)', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb187ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e91caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function named get_telco_data that returns the data from the telco_churn database in SQL. \n",
    "# In your SQL, be sure to join contract_types, internet_service_types, payment_types tables with \n",
    "# the customers table, so that the resulting dataframe contains all the contract, payment, \n",
    "# and internet service options. Obtain your data from the Codeup Data Science Database.\n",
    "\n",
    "def get_telco_data():\n",
    "    from env import user, password, host\n",
    "    url = f'mysql+pymysql://{user}:{password}@{host}/telco_churn'\n",
    "    url = env.get_db_url('telco_churn')\n",
    "    return pd.read_sql('SELECT * FROM customers\\\n",
    "    JOIN telco_churn.contract_types USING(contract_type_id)\\\n",
    "    JOIN telco_churn.internet_service_types USING(internet_service_type_id)\\\n",
    "    JOIN telco_churn.payment_types USING(payment_type_id)', url)\n",
    "\n",
    "get_telco_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969cab2",
   "metadata": {},
   "source": [
    "#### Once you've got your get_titanic_data, get_iris_data, and get_telco_data functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of telco.csv, titanic.csv, or iris.csv. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d90c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titanic_data():\n",
    "    filename = \"titanic.csv\"\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        url = env.get_db_url('titanic_db')\n",
    "        return pd.read_sql('SELECT * FROM passengers', url)\n",
    "    \n",
    "titanic_df = get_titanic_data()\n",
    "titanic_df.to_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iris_data():\n",
    "    filename = \"iris.csv\"\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        from env import user, password, host\n",
    "        url = f'mysql+pymysql://{user}:{password}@{host}/iris_db'\n",
    "        url = env.get_db_url('iris_db')\n",
    "        return pd.read_sql('SELECT * FROM measurements JOIN iris_db.species USING(species_id)', url)\n",
    "\n",
    "iris_df = get_iris_data()\n",
    "iris_df.to_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ffcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_telco_data():\n",
    "    filename = \"telco.csv\"\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    else:\n",
    "        from env import user, password, host\n",
    "        url = f'mysql+pymysql://{user}:{password}@{host}/telco_churn'\n",
    "        url = env.get_db_url('telco_churn')\n",
    "        return pd.read_sql('SELECT * FROM customers\\\n",
    "        JOIN telco_churn.contract_types USING(contract_type_id)\\\n",
    "        JOIN telco_churn.internet_service_types USING(internet_service_type_id)\\\n",
    "        JOIN telco_churn.payment_types USING(payment_type_id)', url)\n",
    "\n",
    "telco_df = get_telco_data()\n",
    "telco_df.to_csv(\"telco.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e185b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use import acquire to get these functions from other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22defd45",
   "metadata": {},
   "source": [
    "## Data Preparation Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436c59c",
   "metadata": {},
   "source": [
    "The end product of this exercise should be the specified functions in a python script named prepare.py. Do these in your classification_exercises.ipynb first, then transfer to the prepare.py file.\n",
    "\n",
    "This work should all be saved in your local classification-exercises repo. Then add, commit, and push your changes.\n",
    "\n",
    "## Using the Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f495c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use the function defined in acquire.py to load the iris data.\n",
    "iris_df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7df7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop the species_id and measurement_id columns.\n",
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['species_id', 'measurement_id']\n",
    "iris_df = iris_df.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rename the species_name column to just species.\n",
    "iris_df = iris_df.rename(columns={'species_name': 'species'})\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede24128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create dummy variables of the species name and concatenate onto the iris dataframe. \n",
    "# (This is for practice, we don't always have to encode the target, but if we used species \n",
    "# as a feature, we would need to encode it).\n",
    "# Using drop_first leaves sex_male, embark_town_Queenstown, and embark_town_Southampton.\n",
    "\n",
    "dummy_df = pd.get_dummies(iris_df[['species']], dummy_na=False, drop_first=True)\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39460e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.concat([iris_df, dummy_df], axis=1)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Create a function named prep_iris that accepts the untransformed iris data, and returns the data\n",
    "# with the transformations above applied.\n",
    "def prep_iris(df):\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #rename column\n",
    "    df = df.rename(columns={'species_name': 'species'})\n",
    "    # Drop columns \n",
    "    columns_to_drop = ['species_id', 'measurement_id']\n",
    "    df = df.drop(columns = columns_to_drop)\n",
    "    # encoded categorical variables\n",
    "    dummy_df = pd.get_dummies(df[['species']], dummy_na=False, drop_first=True)\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4119aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_iris_data()\n",
    "prep_iris(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0ca9a",
   "metadata": {},
   "source": [
    "## Using the Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Use the function defined in acquire.py to load the Titanic data.\n",
    "titanic_df = acquire.get_titanic_data()\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Drop any unnecessary, unhelpful, or duplicated columns.\n",
    "titanic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b67286",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['embarked', 'pclass', 'passenger_id', 'deck']\n",
    "titanic_df = titanic_df.drop(columns = columns_to_drop) \n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical columns. Create dummy variables of the categorical columns and \n",
    "# concatenate them onto the dataframe.\n",
    "dummy_df = pd.get_dummies(titanic_df[['sex', 'class', 'embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate my dummy_df to my data\n",
    "titanic_df = pd.concat([titanic_df, dummy_df], axis=1)\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabd168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a function named prep_titanic that accepts the raw titanic data, and returns the data\n",
    "# with the transformations above applied.\n",
    "def prep_titanic(df):\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # Drop columns \n",
    "    columns_to_drop = ['embarked', 'pclass', 'passenger_id', 'deck']\n",
    "    df = df.drop(columns = columns_to_drop)\n",
    "    # encoded categorical variables\n",
    "    dummy_df = pd.get_dummies(df[['sex', 'class', 'embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_titanic_data()\n",
    "clean_titanic = prep_titanic(df)\n",
    "clean_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1850048",
   "metadata": {},
   "source": [
    "## Using the Telco dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94654019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Use the function defined in acquire.py to load the Telco data.\n",
    "telco_df = acquire.get_telco_data()\n",
    "telco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e4fdf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns\n",
    "# but keeping the corresponding string values, for example.\n",
    "telco_df.info()\n",
    "telco_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d103e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates, None here\n",
    "telco_df = telco_df.drop_duplicates()\n",
    "telco_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unneccessary columns\n",
    "columns_to_drop = ['payment_type_id', 'internet_service_type_id', 'contract_type_id']\n",
    "telco_df = telco_df.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ec2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Encode the categorical columns. Create dummy variables of the categorical columns and concatenate\n",
    "# them onto the dataframe.\n",
    "# Creating a list of our numeric columns\n",
    "numcols = [col for col in telco_df.columns if telco_df[col].dtype != 'O']\n",
    "numcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa89fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of our categorical columns\n",
    "catcols = [col for col in telco_df.columns if telco_df[col].dtype == 'O']\n",
    "catcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2590ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(telco_df[[\n",
    " 'gender',\n",
    " 'partner',\n",
    " 'dependents',\n",
    " 'phone_service',\n",
    " 'multiple_lines',\n",
    " 'online_security',\n",
    " 'online_backup',\n",
    " 'device_protection',\n",
    " 'tech_support',\n",
    " 'streaming_tv',\n",
    " 'streaming_movies',\n",
    " 'paperless_billing',\n",
    " 'churn',\n",
    " 'contract_type',\n",
    " 'internet_service_type',\n",
    " 'payment_type']], dummy_na=False, drop_first=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42194aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a function named prep_telco that accepts the raw telco data, and returns the data with the\n",
    "# transformations above applied.\n",
    "def prep_telco(df):\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # Drop columns \n",
    "    columns_to_drop = ['payment_type_id', 'internet_service_type_id', 'contract_type_id']\n",
    "    df = df.drop(columns = columns_to_drop)\n",
    "    # encoded categorical variables\n",
    "    dummy_df = pd.get_dummies(df[['gender','partner','dependents','phone_service', 'multiple_lines','online_security','online_backup','device_protection','tech_support','streaming_tv','streaming_movies','paperless_billing','churn','contract_type','internet_service_type','payment_type']], dummy_na=False, drop_first=[True, True])\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11341517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea675495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_telco(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec67761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c78d2f",
   "metadata": {},
   "source": [
    "## Split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6637e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Write a function to split your data into train, test and validate datasets. Add this function to prepare.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969abc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% test, 80% train_validate\n",
    "# then of the 80% train_validate: 30% validate, 70% train. \n",
    "\n",
    "train_validate, test = train_test_split(df, test_size=.2, \n",
    "                                        random_state=123, \n",
    "                                        stratify=df.churn)\n",
    "train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                                   random_state=123, \n",
    "                                   stratify=train_validate.churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1606a1f",
   "metadata": {},
   "source": [
    "### 2. Run the function in your notebook on the Iris dataset, returning 3 datasets, train_iris, validate_iris and test_iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd7824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acquire iris data\n",
    "df = acquire.get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8053a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep iris data\n",
    "df = prepare.prep_iris(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a319694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target variable\n",
    "target = 'species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1152c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "#from prepare import my_train_test_split\n",
    "train_iris, validate_iris, test_iris = prepare.my_train_test_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb394f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check training datasets\n",
    "train_iris.shape\n",
    "test_iris.shape\n",
    "validate_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452c478",
   "metadata": {},
   "source": [
    "### 3. Run the function on the Titanic dataset, returning 3 datasets, train_titanic, validate_titanic and test_titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b56a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acquire titanic data\n",
    "df = acquire.get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c851f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep titanic data\n",
    "df = prepare.prep_titanic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8a30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target variable\n",
    "target = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deaabed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "#from prepare import my_train_test_split\n",
    "train_titanic, validate_titanic, test_titanic = prepare.my_train_test_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fecafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check training datasets\n",
    "train_titanic.shape\n",
    "test_titanic.shape\n",
    "validate_titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095d0cf",
   "metadata": {},
   "source": [
    "### 4. Run the function on the Telco dataset, returning 3 datasets, train_telco, validate_telco and test_telco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "543f801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acquire telco data\n",
    "df = acquire.get_telco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a6f9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep telco data\n",
    "df = prepare.prep_telco(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e492adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'churn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4217184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "train_telco, validate_telco, test_telco = prepare.my_train_test_split(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ff84191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4225, 48)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_telco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afea25bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409, 48)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_telco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db87c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409, 48)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_telco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2810ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
